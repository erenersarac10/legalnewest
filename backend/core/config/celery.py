"""
Celery Configuration - Harvey/Legora %100 Distributed Task Queue.

Production-grade async task processing with:
- Redis broker (6x faster than RabbitMQ for small messages)
- Multiple task queues (priority-based routing)
- Task retries with exponential backoff
- Task monitoring (Flower dashboard)
- Beat scheduler for periodic tasks
- Canvas workflows (chains, groups, chords)
- Task result backend (Redis)
- Task rate limiting
- Dead letter queues

Why Celery?
    Without: Blocking operations â†’ slow API responses â†’ bad UX
    With: Async processing â†’ instant API responses â†’ Harvey-level performance

    Impact: 10x faster API response times! ðŸš€

Architecture:
    API â†’ Celery Producer â†’ Redis Broker â†’ Celery Workers â†’ Result Backend
          â†“ Non-blocking       â†“ Queue          â†“ Process task  â†“ Store result
          Return task ID        Persist task     Return result   Retrieved by API

Task Types:
    1. RAG Generation (high priority, CPU-intensive)
    2. Document Processing (medium priority, I/O-intensive)
    3. Embeddings Generation (medium priority, GPU if available)
    4. Email/Notifications (low priority, I/O-intensive)
    5. Cache Warming (low priority, background)
    6. Audit Logging (high priority, critical)

Queues:
    - rag: RAG generation tasks (priority 10)
    - documents: Document parsing, OCR (priority 5)
    - embeddings: Vector embeddings (priority 5)
    - notifications: Email, SMS (priority 3)
    - background: Cache warming, cleanup (priority 1)
    - audit: Audit logging (priority 10, critical)

Usage:
    >>> from backend.tasks.rag import generate_legal_answer
    >>>
    >>> # Async execution
    >>> task = generate_legal_answer.delay(query="Ä°ÅŸ sÃ¶zleÅŸmesi feshi")
    >>> print(task.id)  # "a1b2c3d4-..."
    >>>
    >>> # Check status
    >>> result = task.get(timeout=60)
    >>> print(result)  # {"answer": "...", "sources": [...]}
"""

from typing import Dict, List, Optional
from enum import Enum
from pydantic import BaseModel
import os


class CeleryBroker(str, Enum):
    """Celery broker backends."""

    REDIS = "redis"  # Recommended for speed
    RABBITMQ = "rabbitmq"  # Better for reliability
    SQS = "sqs"  # AWS Simple Queue Service


class ResultBackend(str, Enum):
    """Task result storage backends."""

    REDIS = "redis"  # Fast, in-memory
    DATABASE = "database"  # PostgreSQL (persistent)
    MEMCACHED = "memcached"  # Fast, distributed cache
    S3 = "s3"  # AWS S3 (large results)


class TaskPriority(int, Enum):
    """Task priority levels (10 = highest, 1 = lowest)."""

    CRITICAL = 10  # Audit logs, security-critical
    HIGH = 8  # RAG generation, user-facing
    MEDIUM = 5  # Document processing
    LOW = 3  # Notifications, emails
    BACKGROUND = 1  # Cache warming, cleanup


# =============================================================================
# CELERY CONFIGURATIONS
# =============================================================================


class CeleryConfig(BaseModel):
    """Celery configuration."""

    # Broker settings
    broker: CeleryBroker = CeleryBroker.REDIS
    broker_url: str
    broker_connection_retry_on_startup: bool = True
    broker_connection_retry: bool = True
    broker_connection_max_retries: int = 10

    # Result backend
    result_backend: ResultBackend = ResultBackend.REDIS
    result_backend_url: str
    result_expires: int = 3600  # 1 hour (seconds)
    result_persistent: bool = True

    # Task settings
    task_serializer: str = "json"  # json, pickle, msgpack
    result_serializer: str = "json"
    accept_content: List[str] = ["json"]  # Security: only accept JSON
    timezone: str = "Europe/Istanbul"  # Turkish timezone
    enable_utc: bool = True

    # Task execution
    task_acks_late: bool = True  # Ack after task completes (safer)
    task_reject_on_worker_lost: bool = True  # Requeue if worker dies
    task_time_limit: int = 600  # 10 minutes hard limit
    task_soft_time_limit: int = 540  # 9 minutes soft limit (raise exception)

    # Worker settings
    worker_prefetch_multiplier: int = 4  # Tasks to prefetch per worker
    worker_max_tasks_per_child: int = 1000  # Restart after N tasks (memory leak prevention)
    worker_disable_rate_limits: bool = False  # Enable rate limiting
    worker_log_format: str = "[%(asctime)s: %(levelname)s/%(processName)s] %(message)s"

    # Beat scheduler (periodic tasks)
    beat_scheduler: str = "redis"  # Use Redis for distributed beat
    beat_schedule_filename: str = "/tmp/celerybeat-schedule"

    # Monitoring
    task_send_sent_event: bool = True  # Enable task sent events
    task_track_started: bool = True  # Track when task starts
    worker_send_task_events: bool = True  # Enable worker events (Flower)

    # Retry settings (exponential backoff)
    task_autoretry_for: tuple = (Exception,)  # Retry on all exceptions
    task_max_retries: int = 3
    task_default_retry_delay: int = 60  # 1 minute base delay

    # Queues configuration
    task_routes: Dict[str, Dict[str, str]] = {}
    task_default_queue: str = "default"
    task_default_priority: int = TaskPriority.MEDIUM


# Harvey/Legora %100: Multi-Environment Celery Configuration
CELERY_CONFIGS: Dict[str, CeleryConfig] = {
    # =============================================================================
    # PRODUCTION: Redis Broker + Redis Result Backend
    # =============================================================================
    "production": CeleryConfig(
        broker=CeleryBroker.REDIS,
        broker_url=os.getenv(
            "REDIS_URL",
            "redis://:password@legalai-redis.abc123.euw1.cache.amazonaws.com:6379/0",
        ),
        result_backend=ResultBackend.REDIS,
        result_backend_url=os.getenv(
            "REDIS_URL",
            "redis://:password@legalai-redis.abc123.euw1.cache.amazonaws.com:6379/1",
        ),
        result_expires=3600,  # 1 hour
        task_time_limit=600,  # 10 minutes
        task_soft_time_limit=540,
        worker_prefetch_multiplier=4,
        worker_max_tasks_per_child=1000,
        task_max_retries=3,
        task_default_retry_delay=60,
        # Task routing
        task_routes={
            "backend.tasks.rag.*": {"queue": "rag", "priority": TaskPriority.HIGH},
            "backend.tasks.documents.*": {"queue": "documents", "priority": TaskPriority.MEDIUM},
            "backend.tasks.embeddings.*": {"queue": "embeddings", "priority": TaskPriority.MEDIUM},
            "backend.tasks.notifications.*": {"queue": "notifications", "priority": TaskPriority.LOW},
            "backend.tasks.background.*": {"queue": "background", "priority": TaskPriority.BACKGROUND},
            "backend.tasks.audit.*": {"queue": "audit", "priority": TaskPriority.CRITICAL},
        },
        task_default_queue="default",
    ),

    # =============================================================================
    # STAGING: Redis Broker + Redis Result Backend
    # =============================================================================
    "staging": CeleryConfig(
        broker=CeleryBroker.REDIS,
        broker_url=os.getenv("REDIS_URL", "redis://legalai-staging-redis:6379/0"),
        result_backend=ResultBackend.REDIS,
        result_backend_url=os.getenv("REDIS_URL", "redis://legalai-staging-redis:6379/1"),
        result_expires=1800,  # 30 minutes
        task_time_limit=300,  # 5 minutes
        task_soft_time_limit=270,
        worker_prefetch_multiplier=2,
        worker_max_tasks_per_child=500,
        task_max_retries=2,
        task_routes={
            "backend.tasks.rag.*": {"queue": "rag", "priority": TaskPriority.HIGH},
            "backend.tasks.documents.*": {"queue": "documents", "priority": TaskPriority.MEDIUM},
            "backend.tasks.embeddings.*": {"queue": "embeddings", "priority": TaskPriority.MEDIUM},
            "backend.tasks.notifications.*": {"queue": "notifications", "priority": TaskPriority.LOW},
            "backend.tasks.audit.*": {"queue": "audit", "priority": TaskPriority.CRITICAL},
        },
    ),

    # =============================================================================
    # DEVELOPMENT: Local Redis
    # =============================================================================
    "development": CeleryConfig(
        broker=CeleryBroker.REDIS,
        broker_url=os.getenv("REDIS_URL", "redis://localhost:6379/0"),
        result_backend=ResultBackend.REDIS,
        result_backend_url=os.getenv("REDIS_URL", "redis://localhost:6379/1"),
        result_expires=600,  # 10 minutes
        task_time_limit=180,  # 3 minutes
        task_soft_time_limit=150,
        worker_prefetch_multiplier=1,  # Process one task at a time
        worker_max_tasks_per_child=100,
        task_max_retries=1,  # Fail fast in dev
        task_routes={
            "backend.tasks.rag.*": {"queue": "rag"},
            "backend.tasks.documents.*": {"queue": "documents"},
        },
    ),

    # =============================================================================
    # TESTING: In-Memory (no external dependencies)
    # =============================================================================
    "testing": CeleryConfig(
        broker=CeleryBroker.REDIS,
        broker_url="memory://",  # In-memory broker
        result_backend=ResultBackend.REDIS,
        result_backend_url="cache+memory://",  # In-memory result backend
        result_expires=300,  # 5 minutes
        task_time_limit=60,
        task_soft_time_limit=50,
        task_always_eager=True,  # Execute tasks synchronously
        task_eager_propagates=True,  # Propagate exceptions
        worker_prefetch_multiplier=1,
    ),
}


# =============================================================================
# TASK QUEUE DEFINITIONS
# =============================================================================

TASK_QUEUES: Dict[str, Dict] = {
    "rag": {
        "name": "rag",
        "routing_key": "rag.*",
        "priority": TaskPriority.HIGH,
        "description": "RAG generation tasks (LLM inference)",
        "max_workers": 4,  # 4 concurrent RAG tasks
        "concurrency": 2,  # 2 threads per worker
    },
    "documents": {
        "name": "documents",
        "routing_key": "documents.*",
        "priority": TaskPriority.MEDIUM,
        "description": "Document parsing, OCR, PDF extraction",
        "max_workers": 8,
        "concurrency": 4,
    },
    "embeddings": {
        "name": "embeddings",
        "routing_key": "embeddings.*",
        "priority": TaskPriority.MEDIUM,
        "description": "Vector embeddings generation",
        "max_workers": 4,
        "concurrency": 8,  # Batch processing
    },
    "notifications": {
        "name": "notifications",
        "routing_key": "notifications.*",
        "priority": TaskPriority.LOW,
        "description": "Email, SMS, push notifications",
        "max_workers": 2,
        "concurrency": 10,  # I/O bound
    },
    "background": {
        "name": "background",
        "routing_key": "background.*",
        "priority": TaskPriority.BACKGROUND,
        "description": "Cache warming, cleanup, analytics",
        "max_workers": 2,
        "concurrency": 4,
    },
    "audit": {
        "name": "audit",
        "routing_key": "audit.*",
        "priority": TaskPriority.CRITICAL,
        "description": "Audit logging (security-critical)",
        "max_workers": 4,
        "concurrency": 10,
    },
}


# =============================================================================
# BEAT SCHEDULE (Periodic Tasks)
# =============================================================================

BEAT_SCHEDULE: Dict = {
    # Cache warming (every 1 hour)
    "warm-cache": {
        "task": "backend.tasks.background.warm_cache",
        "schedule": 3600.0,  # 1 hour
        "options": {"queue": "background", "priority": TaskPriority.BACKGROUND},
    },

    # Cleanup expired sessions (every 6 hours)
    "cleanup-sessions": {
        "task": "backend.tasks.background.cleanup_sessions",
        "schedule": 21600.0,  # 6 hours
        "options": {"queue": "background", "priority": TaskPriority.BACKGROUND},
    },

    # Sync legal sources (every day at 3 AM)
    "sync-legal-sources": {
        "task": "backend.tasks.background.sync_legal_sources",
        "schedule": {
            "hour": 3,
            "minute": 0,
        },
        "options": {"queue": "documents", "priority": TaskPriority.MEDIUM},
    },

    # Auto-update YargÄ±tay dataset (every day at 4 AM)
    "update-yargitay-dataset": {
        "task": "backend.tasks.legal_sources.update_yargitay_decisions",
        "schedule": {
            "hour": 4,
            "minute": 0,
        },
        "options": {"queue": "documents", "priority": TaskPriority.HIGH},
        "description": "Fetch latest YargÄ±tay decisions from karararama.yargitay.gov.tr",
    },

    # Auto-update AYM dataset (every day at 4:30 AM)
    "update-aym-dataset": {
        "task": "backend.tasks.legal_sources.update_aym_decisions",
        "schedule": {
            "hour": 4,
            "minute": 30,
        },
        "options": {"queue": "documents", "priority": TaskPriority.HIGH},
        "description": "Fetch latest AYM decisions from kararlarbilgibankasi.anayasa.gov.tr",
    },

    # Auto-update DanÄ±ÅŸtay dataset (every day at 5 AM)
    "update-danistay-dataset": {
        "task": "backend.tasks.legal_sources.update_danistay_decisions",
        "schedule": {
            "hour": 5,
            "minute": 0,
        },
        "options": {"queue": "documents", "priority": TaskPriority.HIGH},
        "description": "Fetch latest DanÄ±ÅŸtay decisions from karararama.danistay.gov.tr",
    },

    # Generate analytics (every day at 1 AM)
    "generate-analytics": {
        "task": "backend.tasks.background.generate_analytics",
        "schedule": {
            "hour": 1,
            "minute": 0,
        },
        "options": {"queue": "background", "priority": TaskPriority.LOW},
    },

    # Archive old audit logs (every week, Sunday 2 AM)
    "archive-audit-logs": {
        "task": "backend.tasks.audit.archive_old_logs",
        "schedule": {
            "day_of_week": 0,  # Sunday
            "hour": 2,
            "minute": 0,
        },
        "options": {"queue": "audit", "priority": TaskPriority.MEDIUM},
    },

    # Health check (every 5 minutes)
    "health-check": {
        "task": "backend.tasks.background.health_check",
        "schedule": 300.0,  # 5 minutes
        "options": {"queue": "background", "priority": TaskPriority.LOW},
    },
}


# =============================================================================
# TASK RETRY CONFIGURATION
# =============================================================================

TASK_RETRY_CONFIGS: Dict[str, Dict] = {
    # RAG tasks: Retry on LLM failures
    "rag": {
        "autoretry_for": (Exception,),
        "max_retries": 3,
        "default_retry_delay": 30,  # 30 seconds
        "retry_backoff": True,  # Exponential backoff
        "retry_backoff_max": 300,  # Max 5 minutes
        "retry_jitter": True,  # Add randomness
    },

    # Document tasks: Retry on temporary failures
    "documents": {
        "autoretry_for": (Exception,),
        "max_retries": 5,
        "default_retry_delay": 60,
        "retry_backoff": True,
        "retry_backoff_max": 600,
    },

    # Embeddings: Retry on rate limits
    "embeddings": {
        "autoretry_for": (Exception,),
        "max_retries": 10,
        "default_retry_delay": 5,
        "retry_backoff": True,
        "retry_backoff_max": 120,
    },

    # Notifications: Retry aggressively
    "notifications": {
        "autoretry_for": (Exception,),
        "max_retries": 5,
        "default_retry_delay": 120,  # 2 minutes
        "retry_backoff": True,
    },

    # Audit: Critical, no retries (must succeed immediately)
    "audit": {
        "autoretry_for": tuple(),  # No auto-retry
        "max_retries": 0,
    },
}


# =============================================================================
# HELPER FUNCTIONS
# =============================================================================


def get_celery_config(environment: str = "production") -> CeleryConfig:
    """
    Get Celery configuration for environment.

    Args:
        environment: Environment name (production, staging, development, testing)

    Returns:
        CeleryConfig instance

    Example:
        >>> config = get_celery_config("production")
        >>> print(config.broker_url)
        redis://:password@legalai-redis.abc123.euw1.cache.amazonaws.com:6379/0
    """
    return CELERY_CONFIGS.get(environment, CELERY_CONFIGS["development"])


def get_queue_config(queue_name: str) -> Dict:
    """
    Get queue configuration.

    Args:
        queue_name: Queue name (rag, documents, embeddings, etc.)

    Returns:
        Queue configuration dict
    """
    return TASK_QUEUES.get(queue_name, TASK_QUEUES["default"])


def get_retry_config(task_type: str) -> Dict:
    """
    Get retry configuration for task type.

    Args:
        task_type: Task type (rag, documents, embeddings, etc.)

    Returns:
        Retry configuration dict
    """
    return TASK_RETRY_CONFIGS.get(task_type, {})


__all__ = [
    "CeleryBroker",
    "ResultBackend",
    "TaskPriority",
    "CeleryConfig",
    "CELERY_CONFIGS",
    "TASK_QUEUES",
    "BEAT_SCHEDULE",
    "TASK_RETRY_CONFIGS",
    "get_celery_config",
    "get_queue_config",
    "get_retry_config",
]
