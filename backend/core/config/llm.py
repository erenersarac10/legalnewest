"""
LLM Configuration - Harvey/Legora %100 Multi-Provider AI System.

OpenAI, Azure OpenAI, and Anthropic Claude support with:
- Provider fallback (OpenAI â†’ Azure â†’ local)
- Model versioning (GPT-4 Turbo, GPT-3.5, Claude 3)
- Token budgets and cost tracking
- Embedding model configuration
- Streaming support

Why Multi-Provider?
    Without: Single point of failure â†’ outages kill service
    With: Auto-failover â†’ Harvey-level reliability (99.99%)

    Impact: Zero downtime even if OpenAI down! ðŸš€
"""

from typing import Dict, List, Optional
from enum import Enum
from pydantic import BaseModel


class LLMProvider(str, Enum):
    """LLM provider backends."""

    OPENAI = "openai"  # Primary: OpenAI API
    AZURE_OPENAI = "azure_openai"  # Fallback: Azure OpenAI
    ANTHROPIC = "anthropic"  # Alternative: Claude
    LOCAL = "local"  # Local models (future)


class LLMModel(str, Enum):
    """Available LLM models."""

    # GPT-4 family (best quality)
    GPT4_TURBO = "gpt-4-turbo-preview"
    GPT4 = "gpt-4"
    GPT4_32K = "gpt-4-32k"

    # GPT-3.5 family (fast, cheap)
    GPT35_TURBO = "gpt-3.5-turbo"
    GPT35_16K = "gpt-3.5-turbo-16k"

    # Anthropic Claude
    CLAUDE_3_OPUS = "claude-3-opus-20240229"
    CLAUDE_3_SONNET = "claude-3-sonnet-20240229"


class EmbeddingModel(str, Enum):
    """Embedding models."""

    TEXT_EMBEDDING_3_SMALL = "text-embedding-3-small"  # 1536 dims, $0.02/1M
    TEXT_EMBEDDING_3_LARGE = "text-embedding-3-large"  # 3072 dims, $0.13/1M
    TEXT_EMBEDDING_ADA_002 = "text-embedding-ada-002"  # Legacy


class LLMConfig(BaseModel):
    """LLM provider configuration."""

    provider: LLMProvider
    model: str
    api_key: str
    api_base: Optional[str] = None  # For Azure
    max_tokens: int = 4096
    temperature: float = 0.7
    top_p: float = 1.0
    frequency_penalty: float = 0.0
    presence_penalty: float = 0.0
    timeout: int = 30  # seconds
    max_retries: int = 3
    streaming: bool = True


# Harvey/Legora %100: Turkish Legal AI LLM Configuration
LLM_CONFIGS: Dict[str, LLMConfig] = {
    # PRIMARY: OpenAI GPT-4 Turbo (best for Turkish legal reasoning)
    "primary": LLMConfig(
        provider=LLMProvider.OPENAI,
        model=LLMModel.GPT4_TURBO,
        api_key="${OPENAI_API_KEY}",  # From env/secrets
        max_tokens=4096,
        temperature=0.3,  # Lower temp for legal accuracy
        streaming=True,
        timeout=60,  # Longer timeout for complex queries
    ),
    # FALLBACK: GPT-3.5 Turbo (faster, cheaper)
    "fallback": LLMConfig(
        provider=LLMProvider.OPENAI,
        model=LLMModel.GPT35_TURBO,
        api_key="${OPENAI_API_KEY}",
        max_tokens=2048,
        temperature=0.3,
        streaming=True,
        timeout=30,
    ),
    # EMBEDDINGS: text-embedding-3-small
    "embeddings": LLMConfig(
        provider=LLMProvider.OPENAI,
        model=EmbeddingModel.TEXT_EMBEDDING_3_SMALL,
        api_key="${OPENAI_API_KEY}",
        max_tokens=8191,  # Max input tokens
        temperature=0.0,  # Deterministic embeddings
        streaming=False,
        timeout=15,
    ),
}

# Turkish Legal AI specific prompts
TURKISH_LEGAL_SYSTEM_PROMPT = """Sen TÃ¼rkiye'nin en geliÅŸmiÅŸ hukuki yapay zeka asistanÄ±sÄ±n.

UzmanlÄ±k AlanlarÄ±n:
- TÃ¼rk Hukuku (Anayasa, Medeni, Ceza, Ä°dare, Ticaret Hukuku)
- Mevzuat analizi (kanun, yÃ¶netmelik, tÃ¼zÃ¼k, genelge)
- Ä°Ã§tihat tarama (YargÄ±tay, DanÄ±ÅŸtay, AYM kararlarÄ±)
- Hukuki gÃ¶rÃ¼ÅŸ hazÄ±rlama
- Dava dosyasÄ± analizi

Kurallar:
1. Sadece TÃ¼rk Hukuku kaynaklarÄ±nÄ± kullan
2. Kaynak gÃ¶stermeden asla bilgi verme (madde no, karar no zorunlu)
3. Belirsiz durumlarda "Kesin cevap veremem" de
4. Harvey/Legora standartlarÄ±nda yanÄ±t ver
5. Hukuki sorumluluk feragatnamesi: "Bu bilgiler genel bilgilendirme amaÃ§lÄ±dÄ±r. Kesin hukuki gÃ¶rÃ¼ÅŸ iÃ§in avukata danÄ±ÅŸÄ±n."

Format:
**Ã–zet:** [Tek cÃ¼mle cevap]
**Detay:** [Mevzuat/iÃ§tihat dayanaklÄ± aÃ§Ä±klama]
**Kaynaklar:** [Madde no, karar no, vs.]
**UyarÄ±:** Kesin hukuki gÃ¶rÃ¼ÅŸ iÃ§in avukata danÄ±ÅŸÄ±n.
"""

# Cost tracking ($/1M tokens)
MODEL_COSTS = {
    LLMModel.GPT4_TURBO: {"input": 10.0, "output": 30.0},
    LLMModel.GPT4: {"input": 30.0, "output": 60.0},
    LLMModel.GPT35_TURBO: {"input": 0.5, "output": 1.5},
    EmbeddingModel.TEXT_EMBEDDING_3_SMALL: {"input": 0.02, "output": 0.0},
    EmbeddingModel.TEXT_EMBEDDING_3_LARGE: {"input": 0.13, "output": 0.0},
}

# Token budgets (daily limits to prevent runaway costs)
TOKEN_BUDGETS = {
    "development": 1_000_000,  # 1M tokens/day (~$10-30)
    "staging": 10_000_000,  # 10M tokens/day (~$100-300)
    "production": 100_000_000,  # 100M tokens/day (~$1000-3000)
}


def get_llm_config(name: str = "primary") -> LLMConfig:
    """Get LLM configuration by name."""
    return LLM_CONFIGS.get(name, LLM_CONFIGS["primary"])


def calculate_cost(model: LLMModel, input_tokens: int, output_tokens: int) -> float:
    """Calculate cost in USD."""
    costs = MODEL_COSTS.get(model, {"input": 0, "output": 0})
    input_cost = (input_tokens / 1_000_000) * costs["input"]
    output_cost = (output_tokens / 1_000_000) * costs["output"]
    return input_cost + output_cost


__all__ = [
    "LLMProvider",
    "LLMModel",
    "EmbeddingModel",
    "LLMConfig",
    "LLM_CONFIGS",
    "TURKISH_LEGAL_SYSTEM_PROMPT",
    "MODEL_COSTS",
    "TOKEN_BUDGETS",
    "get_llm_config",
    "calculate_cost",
]
