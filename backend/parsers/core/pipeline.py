"""
Parsing Pipeline - Harvey/Legora CTO-Level

Orchestrates multi-stage document parsing pipeline.
Combines multiple parsers (structural, semantic, validators) into a single workflow.

Pipeline Stages:
    1. Normalization: Clean and standardize input
    2. Structural Parsing: Extract document structure
    3. Semantic Extraction: Extract semantic information
    4. Enrichment: Add metadata, resolve citations
    5. Validation: Quality checks
    6. Finalization: Generate output

Features:
    - Parallel processing where possible
    - Error recovery and fallback strategies
    - Incremental parsing for large documents
    - Caching for performance

Author: Legal AI Team
Version: 1.0.0
"""

from typing import Any, Dict, List, Optional, Callable
from datetime import datetime
import logging
import asyncio
from concurrent.futures import ThreadPoolExecutor
from time import time

from .canonical_schema import LegalDocument, ParsingResult, DocumentType
from .parser_base import BaseParser, StructuralParser, SemanticExtractor, Validator
from .exceptions import ParserError, ParsingError, ValidationError


logger = logging.getLogger(__name__)


class ParsingPipeline:
    """
    Multi-stage parsing pipeline.
    Coordinates multiple parsers to transform raw input into canonical documents.
    """

    def __init__(
        self,
        name: str = "DefaultPipeline",
        config: Optional[Dict[str, Any]] = None
    ):
        """
        Initialize pipeline.

        Args:
            name: Pipeline name
            config: Pipeline configuration
        """
        self.name = name
        self.config = config or {}

        # Pipeline stages
        self.normalizers: List[Callable] = []
        self.structural_parsers: List[StructuralParser] = []
        self.semantic_extractors: List[SemanticExtractor] = []
        self.enrichers: List[Callable] = []
        self.validators: List[Validator] = []
        self.finalizers: List[Callable] = []

        # Execution settings
        self.parallel = self.config.get("parallel", True)
        self.max_workers = self.config.get("max_workers", 4)
        self.fail_fast = self.config.get("fail_fast", False)

        # Statistics
        self.stats = {
            "total_processed": 0,
            "total_failed": 0,
            "stage_durations": {},
        }

        logger.info(f"Initialized pipeline: {name}")

    # ========================================================================
    # PIPELINE CONFIGURATION
    # ========================================================================

    def add_normalizer(self, normalizer: Callable) -> "ParsingPipeline":
        """
        Add a normalization function.

        Args:
            normalizer: Function that takes raw input and returns normalized input

        Returns:
            Self for chaining
        """
        self.normalizers.append(normalizer)
        logger.debug(f"Added normalizer: {normalizer.__name__}")
        return self

    def add_structural_parser(self, parser: StructuralParser) -> "ParsingPipeline":
        """
        Add a structural parser.

        Args:
            parser: Structural parser instance

        Returns:
            Self for chaining
        """
        self.structural_parsers.append(parser)
        logger.debug(f"Added structural parser: {parser.parser_name}")
        return self

    def add_semantic_extractor(self, extractor: SemanticExtractor) -> "ParsingPipeline":
        """
        Add a semantic extractor.

        Args:
            extractor: Semantic extractor instance

        Returns:
            Self for chaining
        """
        self.semantic_extractors.append(extractor)
        logger.debug(f"Added semantic extractor: {extractor.parser_name}")
        return self

    def add_enricher(self, enricher: Callable) -> "ParsingPipeline":
        """
        Add an enrichment function.

        Args:
            enricher: Function that enriches document

        Returns:
            Self for chaining
        """
        self.enrichers.append(enricher)
        logger.debug(f"Added enricher: {enricher.__name__}")
        return self

    def add_validator(self, validator: Validator) -> "ParsingPipeline":
        """
        Add a validator.

        Args:
            validator: Validator instance

        Returns:
            Self for chaining
        """
        self.validators.append(validator)
        logger.debug(f"Added validator: {validator.parser_name}")
        return self

    def add_finalizer(self, finalizer: Callable) -> "ParsingPipeline":
        """
        Add a finalization function.

        Args:
            finalizer: Function that finalizes document

        Returns:
            Self for chaining
        """
        self.finalizers.append(finalizer)
        logger.debug(f"Added finalizer: {finalizer.__name__}")
        return self

    # ========================================================================
    # PIPELINE EXECUTION
    # ========================================================================

    async def process(
        self,
        source: Any,
        document_type: Optional[DocumentType] = None,
        **kwargs
    ) -> ParsingResult:
        """
        Process a document through the pipeline.

        Args:
            source: Raw input
            document_type: Expected document type
            **kwargs: Additional arguments

        Returns:
            ParsingResult
        """
        start_time = time()
        errors = []
        warnings = []
        context = {"source": source, "document_type": document_type, **kwargs}

        try:
            logger.info(f"[{self.name}] Starting pipeline: {source}")

            # Stage 1: Normalization
            normalized = await self._run_stage(
                "normalization",
                self._normalize,
                source,
                context
            )

            # Stage 2: Structural Parsing
            structured = await self._run_stage(
                "structural_parsing",
                self._parse_structure,
                normalized,
                context
            )

            # Stage 3: Semantic Extraction
            semantics = await self._run_stage(
                "semantic_extraction",
                self._extract_semantics,
                structured,
                context
            )

            # Stage 4: Enrichment
            enriched = await self._run_stage(
                "enrichment",
                self._enrich,
                semantics,
                context
            )

            # Stage 5: Validation
            validated, val_errors, val_warnings = await self._run_stage(
                "validation",
                self._validate,
                enriched,
                context
            )
            errors.extend(val_errors)
            warnings.extend(val_warnings)

            # Stage 6: Finalization
            final_document = await self._run_stage(
                "finalization",
                self._finalize,
                validated,
                context
            )

            # Success
            duration_ms = (time() - start_time) * 1000
            self.stats["total_processed"] += 1

            logger.info(f"[{self.name}] Pipeline complete: {duration_ms:.2f}ms")

            return ParsingResult(
                success=True,
                document=final_document,
                parser_name=self.name,
                parsing_duration_ms=duration_ms,
                confidence_score=self._calculate_confidence(final_document, errors),
                completeness_score=self._calculate_completeness(final_document),
                errors=errors,
                warnings=warnings,
            )

        except Exception as e:
            self.stats["total_failed"] += 1
            duration_ms = (time() - start_time) * 1000

            logger.exception(f"[{self.name}] Pipeline failed: {str(e)}")

            return ParsingResult(
                success=False,
                document=None,
                parser_name=self.name,
                parsing_duration_ms=duration_ms,
                confidence_score=0.0,
                completeness_score=0.0,
                errors=[str(e)] + errors,
                warnings=warnings,
            )

    async def _run_stage(
        self,
        stage_name: str,
        stage_func: Callable,
        input_data: Any,
        context: Dict[str, Any]
    ) -> Any:
        """
        Run a pipeline stage and track timing.

        Args:
            stage_name: Name of stage
            stage_func: Function to execute
            input_data: Input to stage
            context: Pipeline context

        Returns:
            Stage output
        """
        start = time()
        try:
            logger.debug(f"[{self.name}] Running stage: {stage_name}")
            result = await stage_func(input_data, context)
            duration = (time() - start) * 1000

            # Track stage timing
            if stage_name not in self.stats["stage_durations"]:
                self.stats["stage_durations"][stage_name] = []
            self.stats["stage_durations"][stage_name].append(duration)

            logger.debug(f"[{self.name}] Stage {stage_name} complete: {duration:.2f}ms")
            return result

        except Exception as e:
            logger.error(f"[{self.name}] Stage {stage_name} failed: {str(e)}")
            if self.fail_fast:
                raise
            return input_data  # Return input unchanged

    # ========================================================================
    # STAGE IMPLEMENTATIONS
    # ========================================================================

    async def _normalize(self, source: Any, context: Dict[str, Any]) -> Any:
        """Apply all normalizers."""
        normalized = source
        for normalizer in self.normalizers:
            normalized = normalizer(normalized, context)
        return normalized

    async def _parse_structure(self, normalized: Any, context: Dict[str, Any]) -> Dict[str, Any]:
        """Apply all structural parsers."""
        if not self.structural_parsers:
            return {"raw": normalized}

        if self.parallel and len(self.structural_parsers) > 1:
            # Run parsers in parallel
            tasks = [parser.parse(normalized) for parser in self.structural_parsers]
            results = await asyncio.gather(*tasks, return_exceptions=True)

            # Merge results (use first successful)
            for result in results:
                if isinstance(result, ParsingResult) and result.success:
                    return result.document.dict() if result.document else {}

        # Sequential fallback
        for parser in self.structural_parsers:
            result = parser.parse(normalized)
            if result.success and result.document:
                return result.document.dict()

        return {"raw": normalized}

    async def _extract_semantics(self, structured: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
        """Apply all semantic extractors."""
        if not self.semantic_extractors:
            return structured

        text = structured.get("full_text", "")

        if self.parallel and len(self.semantic_extractors) > 1:
            # Run extractors in parallel
            tasks = [extractor.extract(text) for extractor in self.semantic_extractors]
            results = await asyncio.gather(*tasks, return_exceptions=True)

            # Merge semantic data
            for i, result in enumerate(results):
                if not isinstance(result, Exception):
                    extractor_name = self.semantic_extractors[i].parser_name
                    structured[extractor_name] = result

        else:
            # Sequential
            for extractor in self.semantic_extractors:
                try:
                    result = extractor.extract(text)
                    structured[extractor.parser_name] = result
                except Exception as e:
                    logger.warning(f"Extractor {extractor.parser_name} failed: {str(e)}")

        return structured

    async def _enrich(self, semantics: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
        """Apply all enrichers."""
        enriched = semantics
        for enricher in self.enrichers:
            enriched = enricher(enriched, context)
        return enriched

    async def _validate(self, enriched: Dict[str, Any], context: Dict[str, Any]) -> tuple:
        """Apply all validators."""
        errors = []
        warnings = []

        for validator in self.validators:
            try:
                # Convert dict to document if needed
                if isinstance(enriched, dict):
                    # Validator expects LegalDocument, skip or implement conversion
                    logger.warning(f"Validator {validator.parser_name} skipped (dict input)")
                    continue

                val_errors, val_warnings = validator.validate_document(enriched)
                errors.extend(val_errors)
                warnings.extend(val_warnings)

            except Exception as e:
                logger.warning(f"Validator {validator.parser_name} failed: {str(e)}")
                warnings.append(f"Validator {validator.parser_name} failed: {str(e)}")

        return enriched, errors, warnings

    async def _finalize(self, validated: Any, context: Dict[str, Any]) -> LegalDocument:
        """Apply all finalizers."""
        finalized = validated
        for finalizer in self.finalizers:
            finalized = finalizer(finalized, context)

        # Convert to LegalDocument if still dict
        if isinstance(finalized, dict):
            return LegalDocument(**finalized)

        return finalized

    # ========================================================================
    # UTILITY METHODS
    # ========================================================================

    def _calculate_confidence(self, document: Optional[LegalDocument], errors: List[str]) -> float:
        """Calculate overall confidence."""
        if errors:
            return 0.0
        if document and hasattr(document, "metadata"):
            return document.metadata.confidence_score
        return 1.0

    def _calculate_completeness(self, document: Optional[LegalDocument]) -> float:
        """Calculate completeness."""
        if not document:
            return 0.0

        filled = 0
        total = 5  # title, full_text, metadata, clauses, citations

        if document.title:
            filled += 1
        if document.full_text:
            filled += 1
        if document.metadata:
            filled += 1
        if document.clauses:
            filled += 1
        if document.citations:
            filled += 1

        return filled / total

    def get_stats(self) -> Dict[str, Any]:
        """Get pipeline statistics."""
        stats = dict(self.stats)

        # Calculate average stage durations
        avg_durations = {}
        for stage, durations in stats["stage_durations"].items():
            if durations:
                avg_durations[stage] = sum(durations) / len(durations)

        stats["avg_stage_durations"] = avg_durations
        return stats


# ============================================================================
# FACTORY FUNCTIONS
# ============================================================================

def create_statute_pipeline() -> ParsingPipeline:
    """
    Create a pipeline optimized for statutes (Kanun, YÃ¶netmelik).

    Returns:
        Configured pipeline
    """
    pipeline = ParsingPipeline(name="StatutePipeline")

    # Add stages specific to statute parsing
    # (actual parsers would be imported and added here)

    return pipeline


def create_decision_pipeline() -> ParsingPipeline:
    """
    Create a pipeline optimized for court decisions.

    Returns:
        Configured pipeline
    """
    pipeline = ParsingPipeline(name="DecisionPipeline")

    # Add stages specific to decision parsing

    return pipeline


__all__ = [
    "ParsingPipeline",
    "create_statute_pipeline",
    "create_decision_pipeline",
]
